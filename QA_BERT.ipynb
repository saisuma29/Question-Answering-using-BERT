{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "QA_BERT",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3be127d01c084978b64e1d2e2bf0ccb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_58d2d155b66641f18d553951c1ca51d8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c1fc74268afe423492dda28f540a8052",
              "IPY_MODEL_34023b9fc080495ead185d7d49e104fd"
            ]
          }
        },
        "58d2d155b66641f18d553951c1ca51d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1fc74268afe423492dda28f540a8052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de9f205152a44b849ac3869b16bda6a7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eecc57f04b6b44b5a384d3129bcaa05d"
          }
        },
        "34023b9fc080495ead185d7d49e104fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b88d03054a548358e07736e9b0297f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:39&lt;00:00, 5.82kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0586e275e7b64d80b563f025187c3234"
          }
        },
        "de9f205152a44b849ac3869b16bda6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eecc57f04b6b44b5a384d3129bcaa05d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b88d03054a548358e07736e9b0297f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0586e275e7b64d80b563f025187c3234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "473e84af145c45dab5132a9dfe992045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ebc7e70cec7b498a8e99de0862191aa2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1edadca04e164e5db0f46f315abca6ed",
              "IPY_MODEL_7df3614c5f284945bcf926cf18fc999b"
            ]
          }
        },
        "ebc7e70cec7b498a8e99de0862191aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1edadca04e164e5db0f46f315abca6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_daa93fca3727493caecee10f96393d0d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ea63258a1814691b2ce974833c41d4b"
          }
        },
        "7df3614c5f284945bcf926cf18fc999b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bad208747bef41038904f939fcc0d576",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:23&lt;00:00, 20.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4db3991e3e8a415886cf539b63c51e8c"
          }
        },
        "daa93fca3727493caecee10f96393d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ea63258a1814691b2ce974833c41d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bad208747bef41038904f939fcc0d576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4db3991e3e8a415886cf539b63c51e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "408c2db619674c14a6c1a0e4382b3d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39ffeb21145142b09f6805f210623126",
              "IPY_MODEL_0aa750824516474590a96906408d85d6"
            ],
            "layout": "IPY_MODEL_56f3ba4d99f849929621a3eaacf709f7"
          }
        },
        "39ffeb21145142b09f6805f210623126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b51506652dae494ab794a863912cf417",
            "max": 433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_641a0897725c47bea5577844f70234c5",
            "value": 433
          }
        },
        "0aa750824516474590a96906408d85d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e807f15c908b4c0cb0023a82732cd6a9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bc0a5de7a13344c3b241072e578f2fd0",
            "value": " 433/433 [00:01&lt;00:00, 391B/s]"
          }
        },
        "56f3ba4d99f849929621a3eaacf709f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51506652dae494ab794a863912cf417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641a0897725c47bea5577844f70234c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e807f15c908b4c0cb0023a82732cd6a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc0a5de7a13344c3b241072e578f2fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9bd3147346d46f880a2487265b125c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c28a9bd3b8c34fefbee8d23279a7562c",
              "IPY_MODEL_d8e405936fa54fba9e8b6484850d2b7b"
            ],
            "layout": "IPY_MODEL_7b581d1e8cdf4a5fb44da746f6d52ac3"
          }
        },
        "c28a9bd3b8c34fefbee8d23279a7562c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_519cab4e2e4b4a78a924049d8639cb0c",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_970cf262def34ae98b7c1e99cc18c739",
            "value": 440473133
          }
        },
        "d8e405936fa54fba9e8b6484850d2b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8910da74e0df43a38b7f7e1f3c8213ff",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_02e56b0a3e9e494eb4365bf695dfdb1b",
            "value": " 440M/440M [00:18&lt;00:00, 24.1MB/s]"
          }
        },
        "7b581d1e8cdf4a5fb44da746f6d52ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "519cab4e2e4b4a78a924049d8639cb0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "970cf262def34ae98b7c1e99cc18c739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8910da74e0df43a38b7f7e1f3c8213ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e56b0a3e9e494eb4365bf695dfdb1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6TqQolDV1_P"
      },
      "source": [
        "# <b> Deep Learning for NLP- Final Project- Question Answering </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rSBwKaO5k2b"
      },
      "source": [
        "\n",
        "\n",
        "# <b> Team Members<br>\n",
        "### <b> Sai Suma Dodda- sdodda4@uic.edu, UIN - 672075210\n",
        "### <b> Vinay Kaushik Kammara- vkamma2@uic.edu, UIN- 659954836\n",
        "### <b> Manasa Kandimalla - mkandi3@uic.edu, UIN - 654992443\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp_LuGHUUxup"
      },
      "source": [
        "#Brief description of the notebook \n",
        "\n",
        "This notebook is written in Google Colab. At one point, you would need to mount your drive to import the model and the json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVxqxj0Gp6pH"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcNsVqtNzUXn",
        "outputId": "24938b37-757b-4dd2-981b-1df4831d815d"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec  8 02:41:45 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dba2at7DYgjZ"
      },
      "source": [
        "# Mounting Google Drive \n",
        "\n",
        "Here we mount the google drive to import the model and json file since the size of the model file is large and takes time to upload.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcrWxDvBNLZa",
        "outputId": "22727b9e-93a7-4001-89fb-70bc2dc63111"
      },
      "source": [
        "# Mounting the drive in order to import the pytorch model\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkoqhoBC8oe4"
      },
      "source": [
        "# Upload data set\n",
        "\n",
        "Please upload the json files and execute the below cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHhTUIY9sej"
      },
      "source": [
        "# Upload the files dev-1.1-TA.json and train-1.1-TA.json directly to colab using the upload to session storage\n",
        "\n",
        "! mkdir data\n",
        "! mv /content/dev-v1.1-TA.json data\n",
        "! mv /content/train-v1.1-TA.json data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCdimYo-qa2Z",
        "outputId": "155f2467-4f8d-4df7-d436-f7897e866bcf"
      },
      "source": [
        "# Installing Transformers module \n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 29.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=cbf4f1b7a2676dc35a2e30a64b6c48f031bf8d2bc3f8e488d36a707de53efa7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z34Bp46TY4Z5"
      },
      "source": [
        "# Model Initialization\n",
        "\n",
        "Model has been declared below. <br>\n",
        "\n",
        "For model forward function,The arguments are specified as below <br>\n",
        "        \n",
        "**input_ids** -- input_ids is a tensor \n",
        "            in shape (batch_size, sequence_length)<br>\n",
        "**attention_mask** -- attention_mask is a tensor\n",
        "            in shape (batch_size, sequence_length)<br>\n",
        "**token_type_ids** -- token_type ids is a tensor\n",
        "            in shape (batch_size, sequence_length)\n",
        "\n",
        "Reference link - https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForQuestionAnswering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeWFFR1b8Qzt"
      },
      "source": [
        "# Declaring the model and initialising the functions\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
        "\n",
        "class BertForSquad(BertPreTrainedModel):\n",
        "    def __init__(self, config: BertConfig):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels # [0, 1] (start or end)\n",
        "        self.bert = BertModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels) # TODO: Not a separate FFN ? (For Start_FFN and End_FFN)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self,input_ids,attention_mask: torch.Tensor,token_type_ids: torch.Tensor):\n",
        "        output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            )\n",
        "\n",
        "        sequence_output = output[0] # the last hidden state (batch, sequence_length, hidden_size)\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        outputs = (start_logits, end_logits) \n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_a0gkyIbfwj"
      },
      "source": [
        "# Data Preprocessing and Testing functions <br>\n",
        "\n",
        "Squad DataSet: An interface implemented for the Stanford question Answering Dataset. <br>\n",
        "\n",
        "Squad Dataset indexing : The argument passed is the index here. Each question has a unique index. This returns id of the question which is saved in the json, corresponding context of the quesion, answer and the start position of the answer in terms of character indexing level. <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502,
          "referenced_widgets": [
            "3be127d01c084978b64e1d2e2bf0ccb0",
            "58d2d155b66641f18d553951c1ca51d8",
            "c1fc74268afe423492dda28f540a8052",
            "34023b9fc080495ead185d7d49e104fd",
            "de9f205152a44b849ac3869b16bda6a7",
            "eecc57f04b6b44b5a384d3129bcaa05d",
            "5b88d03054a548358e07736e9b0297f1",
            "0586e275e7b64d80b563f025187c3234",
            "473e84af145c45dab5132a9dfe992045",
            "ebc7e70cec7b498a8e99de0862191aa2",
            "1edadca04e164e5db0f46f315abca6ed",
            "7df3614c5f284945bcf926cf18fc999b",
            "daa93fca3727493caecee10f96393d0d",
            "9ea63258a1814691b2ce974833c41d4b",
            "bad208747bef41038904f939fcc0d576",
            "4db3991e3e8a415886cf539b63c51e8c"
          ]
        },
        "id": "Znp_Azx89AY5",
        "outputId": "529a02b1-12f0-4762-ae6f-0b63b768f36f"
      },
      "source": [
        "# Dataset declaration\n",
        "# This cell consists of tokenisation and data preprocessing\n",
        "\n",
        "from typing import List, Tuple, Any, Dict, Union\n",
        "from multiprocessing import Process\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizerFast\n",
        "from tqdm import trange\n",
        "\n",
        "# Function to remove spaces\n",
        "def _is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "class SquadExample(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        qas_id,\n",
        "        question_idx,\n",
        "        question_text,\n",
        "        context_text,\n",
        "        answer_text,\n",
        "        start_pos_char,\n",
        "        title,\n",
        "        answers=[],\n",
        "        is_impossible=False, # Considering the non-answerable case\n",
        "    ):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_idx = question_idx\n",
        "        self.question_text = question_text\n",
        "        self.context_text = context_text\n",
        "        self.answer_text = answer_text\n",
        "        self.title = title\n",
        "        self.is_impossible = is_impossible\n",
        "        self.answers = answers\n",
        "        self.start_pos_char = start_pos_char\n",
        "        self.start_pos, self.end_pos = 0, 0\n",
        "\n",
        "        doc_tokens = []\n",
        "        char_to_word_offset = []\n",
        "        prev_is_whitespace = True\n",
        "\n",
        "def _create_sample(input_data, set_type):\n",
        "    is_training = set_type == \"train\"\n",
        "    question_idx = 0\n",
        "    examples = []\n",
        "    for page in tqdm(input_data): # The number of Wikipedia pages\n",
        "        title = page[\"title\"]\n",
        "        for paragraph in page[\"paragraphs\"]:\n",
        "            context_text = paragraph[\"context\"]\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                qas_id = qa[\"id\"]\n",
        "                question_text = qa[\"question\"]\n",
        "                if question_idx != 0 :\n",
        "                    question_idx += 1 # Assign unique index for each question\n",
        "                start_pos_char = None\n",
        "                answer_txt = None\n",
        "                answers = []\n",
        "                \n",
        "                if \"is_impossible\" in qa:\n",
        "                    is_impossible = qa[\"is_impossible\"]\n",
        "                else:\n",
        "                    is_impossible = False\n",
        "                \n",
        "                if not is_impossible:\n",
        "                    if is_training:\n",
        "                        answer = qa[\"answers\"][0]\n",
        "                        answer_text = answer[\"text\"]\n",
        "                        start_pos_char = answer[\"answer_start\"]\n",
        "                    else:\n",
        "                        answers = qa[\"answers\"]\n",
        "\n",
        "                    example = SquadExample(\n",
        "                        qas_id=qas_id,\n",
        "                        question_idx=question_idx,\n",
        "                        question_text=question_text,\n",
        "                        context_text=context_text,\n",
        "                        answer_text=answer_text,\n",
        "                        start_pos_char=start_pos_char,\n",
        "                        title=title,\n",
        "                        is_impossible=is_impossible,\n",
        "                        answers=answers,\n",
        "                    )\n",
        "                    examples.append(example)\n",
        "    print('Dataset length: ', len(examples))\n",
        "    return examples               \n",
        "\n",
        "def get_train_data(data_dir, filename=None):\n",
        "    if data_dir == None:\n",
        "        data_dir = \"\"\n",
        "    with open(os.path.join(data_dir, filename if data_dir is not None else filename), \"r\", encoding=\"utf-8\") as fp:\n",
        "        data = json.load(fp)[\"data\"]\n",
        "        dataset = _create_sample(data, \"train\")\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "class SquadDataset(object):\n",
        "    # Implementing the interface\n",
        "    def __init__(self, json_file: str='data/train-v1.1-TA.json'):\n",
        "        \"\"\" Squad Dataset Initializer\n",
        "        You can use this part as you like.\n",
        "        Load the given json file properly and provide a simple interface.\n",
        "        Arguments:\n",
        "        json_file -- the name of the json file which have to be processed.\n",
        "        \"\"\"\n",
        "        \n",
        "        with open(json_file) as fp:\n",
        "            self.data = json.load(fp)[\"data\"]\n",
        "            print(\"Extracting data samples from raw SQuAD json file\")\n",
        "            self.dataset = get_train_data(data_dir=\"\", filename=json_file)\n",
        "        \n",
        "    # Returns the length of the dataset\n",
        "    def __len__(self) -> int:\n",
        "        length = len(self.dataset)\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        \"\"\" Squad Dataset Indexing\n",
        "        Arguments:\n",
        "        index -- The index of the question\n",
        "                 Each question should have a unique index number in the order.\n",
        "        Return:\n",
        "        id_str -- The id of the question, which is saved in the json file.\n",
        "                  You can find it when you check the file structure.\n",
        "                  This id is needed when evaluating your model.\n",
        "        context -- The corresponding context of the question.\n",
        "        question -- The question\n",
        "        answer -- The corresponding answer of the question.\n",
        "                  Just select the first answer of the answers list.\n",
        "        start_pos -- The position of the answer in terms of character indexing level.\n",
        "                     This information is also saved in the file in raw.\n",
        "        \"\"\"\n",
        "        \n",
        "        sample = self.dataset[index]\n",
        "        id_str = sample.qas_id\n",
        "        context = sample.context_text\n",
        "        question = sample.question_text\n",
        "        answer = sample.answer_text\n",
        "        start_pos = sample.start_pos_char\n",
        "        \n",
        "\n",
        "        sample: Dict[str, Any] = {\n",
        "            'id': id_str,\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'start_pos': start_pos\n",
        "        }\n",
        "\n",
        "        return sample\n",
        "\n",
        "def squad_features(\n",
        "    context: str,\n",
        "    question: str,\n",
        "    answer: Union[str, None],\n",
        "    start_char_pos: Union[int, None],\n",
        "    tokenizer: BertTokenizerFast\n",
        ") -> Tuple[List[int], List[int], int, int]:\n",
        "    \"\"\" Squad feature extractor\n",
        "    Implement the feature extractor from a Squad sample for your model\n",
        "    Return values should follow [CLS + question + SEP + context + SEP] form.\n",
        "    In addition, because start_char_pos is based on character index, you should convert it to proper token index.\n",
        "    Check the test cases to know the functionality in detail.\n",
        "    Note: input_ids and token_type_ids follows the transfomer library documentation \n",
        "    https://huggingface.co/transformers/glossary.html\n",
        "    Arguments:\n",
        "    context -- Context string\n",
        "    question -- Question string\n",
        "    answer -- Answer string. If the answer is None, return None for start_token_pos and end_token_pos\n",
        "    start_char_pos -- Character index which the answer starts from in the context.\n",
        "                      If the answer is None, this argument is also None.\n",
        "    tokenizer -- Tokenizer to encode text strings.\n",
        "                 Explanation: https://huggingface.co/transformers/model_doc/bert.html#berttokenizerfast\n",
        "    Returns:\n",
        "    input_ids -- Input ids\n",
        "    token_type_ids -- Token type ids\n",
        "    start_token_pos -- Token index which the answer starts from in the input_ids list. \n",
        "                       None if no answer is given.\n",
        "    end_token_pos -- Token index which the answer ends by in the input_ids list.\n",
        "                     This includes the last token which located in the index.\n",
        "                     None if no answer is given.\n",
        "    \"\"\"\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(question, context)\n",
        "    input_ids = encoded_dict[\"input_ids\"]\n",
        "    token_type_ids = encoded_dict[\"token_type_ids\"]\n",
        "    input_ids_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    \n",
        "    if answer is None and start_char_pos is None:\n",
        "        start_token_pos = None\n",
        "        end_token_pos = None\n",
        "        return input_ids, token_type_ids, start_token_pos, end_token_pos\n",
        "\n",
        "    start_token_pos, end_token_pos = 0, 0\n",
        "    start_token_pos += token_type_ids.count(0)\n",
        "    start_token_pos += len(tokenizer.tokenize(context[:start_char_pos]))\n",
        "    end_token_pos += len(tokenizer.tokenize(answer)) + start_token_pos - 1\n",
        "    # Extract tokenized answer part only\n",
        "    tokenized_answer = \" \".join(tokenizer.convert_ids_to_tokens(input_ids[start_token_pos : end_token_pos + 1]))\n",
        "\n",
        "    subword_prefix_original = \"##\" if \"##\" in tokenized_answer else \"\"\n",
        "    subword_prefix = \"##\"\n",
        "    tokenized_answer = tokenized_answer.replace('#', '')\n",
        "    if tokenized_answer != answer.lower() and start_token_pos == end_token_pos and answer in tokenized_answer:\n",
        "\n",
        "        # A single word but different subword tokenization case\n",
        "        new_subword_list = [subword_prefix_original + tokenized_answer[:len(answer)], subword_prefix + tokenized_answer[len(answer):]]\n",
        "        input_ids = input_ids[:start_token_pos] + tokenizer.convert_tokens_to_ids(new_subword_list) + input_ids[end_token_pos + 1 :]\n",
        "        token_type_ids.append(1)\n",
        "    assert len(input_ids) == len(token_type_ids)\n",
        "    \n",
        "\n",
        "    return input_ids, token_type_ids, start_token_pos, end_token_pos\n",
        "\n",
        "    \n",
        "########################################################\n",
        "# Helper functions below. DO NOT MODIFY!               #\n",
        "# Read helper classes to implement your code properly! #\n",
        "########################################################\n",
        "\n",
        "class SquadFeatureDataset(object):\n",
        "    \"\"\" Squad Feature Dataset\n",
        "    The wrapper class for the squad_feature function\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset: SquadDataset, bert_type: str, lazy=False, return_sample=False, eval=False):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(bert_type)\n",
        "        self.return_sample = return_sample\n",
        "        self.eval = eval\n",
        "\n",
        "        if not lazy:\n",
        "            self.lazy = True\n",
        "            self.dataset = [self[index] for index in trange(0, len(self.dataset), desc=\"Preprocessing\")]\n",
        "\n",
        "        self.lazy = lazy\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not self.lazy:\n",
        "            return self.dataset[index]\n",
        "        \n",
        "        sample = self.dataset[index]\n",
        "        context = sample['context']\n",
        "        question = sample['question']\n",
        "        answer = sample['answer']\n",
        "        start_pos = sample['start_pos']\n",
        "\n",
        "        if self.eval:\n",
        "            out = squad_features(context, question, None, None, self.tokenizer)\n",
        "        else:\n",
        "            out = squad_features(context, question, answer, start_pos, self.tokenizer)\n",
        "\n",
        "        if self.return_sample:\n",
        "            out = (out, sample)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Testing\n",
        "\n",
        "def test_wrapper_fn(dataset):\n",
        "    [dataset[sample_id]['id'] for sample_id in range(0, len(dataset))]\n",
        "\n",
        "def test_squadDataSet(dataset):\n",
        "    print(\"Dataset Test Cases\")\n",
        "    # First test\n",
        "    assert len(dataset) == 87474, \\\n",
        "        \"Dataset has some missing or duplicated samples.\"\n",
        "    print(\"First test passed\")\n",
        "\n",
        "    # Second test\n",
        "    expected = {\n",
        "        'id': '5733be284776f41900661182',\n",
        "        'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
        "        'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
        "        'answer': 'Saint Bernadette Soubirous',\n",
        "        'start_pos': 515\n",
        "    }\n",
        "    assert dataset[0] == expected, \\\n",
        "        \"Indexing form does not match the expected result.\"\n",
        "    print(\"Second test passed\")\n",
        "\n",
        "    # Third test\n",
        "    expected = {\n",
        "        'id': '57302039b2c2fd14005688db', \n",
        "        'context': 'On February 29, 2012, Microsoft released Windows 8 Consumer Preview, the beta version of Windows 8, build 8250. Alongside other changes, the build removed the Start button from the taskbar for the first time since its debut on Windows 95; according to Windows manager Chaitanya Sareen, the Start button was removed to reflect their view that on Windows 8, the desktop was an \"app\" itself, and not the primary interface of the operating system. Windows president Steven Sinofsky said more than 100,000 changes had been made since the developer version went public. The day after its release, Windows 8 Consumer Preview had been downloaded over one million times. Like the Developer Preview, the Consumer Preview expired on January 15, 2013.', \n",
        "        'question': 'When was the beta version of Windows 8 made available to the public?', \n",
        "        'answer': 'February 29, 2012', \n",
        "        'start_pos': 3\n",
        "    }\n",
        "    assert dataset[80000] == expected, \\\n",
        "        \"Indexing result does not match the expected result.\"\n",
        "    print(\"Third test passed\")\n",
        "\n",
        "    result = True\n",
        "\n",
        "    process = Process(target=test_wrapper_fn, args=(dataset,))\n",
        "    process.start()\n",
        "    process.join(timeout=1)\n",
        "\n",
        "    if process.is_alive():\n",
        "        process.terminate()\n",
        "        process.join()\n",
        "        result = False\n",
        "\n",
        "    # Forth test\n",
        "    assert result, \\\n",
        "        \"Indexing is too slow.\"\n",
        "    print(\"Forth test passed\")\n",
        "\n",
        "    # Fifth test\n",
        "    assert not process.exitcode, \\\n",
        "        \"Error while indexing the dataset.\"\n",
        "    print(\"Fifth test passed\")\n",
        "\n",
        "    print(\"All tests passed\")\n",
        "\n",
        "def test_squadFeatureExtractor(dataset):\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # First test\n",
        "    context = 'This is a sample context. BERT will find the answer words in the context by pointing the start and end token positions.'\n",
        "    question = 'Where are the answer words?'\n",
        "    answer = 'in the context'\n",
        "    start_pos = context.find(answer)\n",
        "    input_ids, token_type_ids, start_pos, end_pos = squad_features(context, question, answer, start_pos, tokenizer)\n",
        "\n",
        "    assert tokenizer.convert_ids_to_tokens(input_ids) == \\\n",
        "        ['[CLS]', 'where', 'are', 'the', 'answer', 'words', '?', '[SEP]', \\\n",
        "         'this', 'is', 'a', 'sample', 'context', '.', \\\n",
        "         'bert', 'will', 'find', 'the', 'answer', 'words', 'in', 'the', 'context', \\\n",
        "         'by', 'pointing', 'the', 'start', 'and', 'end', 'token', 'positions', '.', '[SEP]'], \\\n",
        "             \"Your tokenized result does not match the expected result.\"\n",
        "\n",
        "    assert token_type_ids == \\\n",
        "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \\\n",
        "        \"Sentence type ids do not math the expected result\"\n",
        "\n",
        "    assert tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1]) == ['in', 'the', 'context'], \\\n",
        "        \"The start and end tokens do not point the answer position.\"\n",
        "\n",
        "    print(\"First test passed!\")\n",
        "\n",
        "    # Second test\n",
        "    context = 'Sometimes, the answer could be subwords so you may need to split them manually.'\n",
        "    question = 'What should the answer consist of'\n",
        "    answer = 'word'\n",
        "    start_pos = context.find(answer)\n",
        "    input_ids, token_type_ids, start_pos, end_pos = squad_features(context, question, answer, start_pos, tokenizer)\n",
        "    \n",
        "    assert tokenizer.convert_ids_to_tokens(input_ids) == \\\n",
        "        ['[CLS]', 'what', 'should', 'the', 'answer', 'consist', 'of', '[SEP]',\n",
        "         'sometimes', ',', 'the', 'answer', 'could', 'be', 'sub', '##word', '##s',\n",
        "         'so', 'you', 'may', 'need', 'to', 'split', 'them', 'manually', '.', '[SEP]'], \\\n",
        "             \"Your tokenized result does not match the expected result.\"\n",
        "\n",
        "    assert token_type_ids == \\\n",
        "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \\\n",
        "        \"Sentence type ids do not math the expected result\"\n",
        "\n",
        "    assert tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1]) == ['##word'], \\\n",
        "        \"The start and end tokens do not point the answer position.\"\n",
        "\n",
        "    print(\"Second test passed!\")\n",
        "\n",
        "    # Third test\n",
        "    context = 'When the answer is not given, you should return None for start_pos and end_pos.'\n",
        "    question = 'This test case does not need a question'\n",
        "    input_ids, token_type_ids, start_pos, end_pos = squad_features(context, question, None, None, tokenizer)\n",
        "\n",
        "    assert len(input_ids) == 33, \\\n",
        "        \"Tokenized result does not match the expected result.\"\n",
        "    \n",
        "    assert start_pos is None and end_pos is None, \\\n",
        "        \"Return None for start_pos and end_pos when the answer is not given.\"\n",
        "\n",
        "    print(\"Third test passed\")\n",
        "\n",
        "    # Forth test\n",
        "    sample = dataset[0]\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "    answer = sample['answer']\n",
        "    start_pos = sample['start_pos']\n",
        "\n",
        "    input_ids, token_type_ids, start_pos, end_pos = squad_features(context, question, answer, start_pos, tokenizer)\n",
        "\n",
        "    assert len(input_ids) == 176, \\\n",
        "        \"Tokenized result does not match the expected result.\"\n",
        "\n",
        "    assert tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1]) == tokenizer.tokenize(answer), \\\n",
        "        \"The start and end tokens do not point to the answer position.\"\n",
        "\n",
        "    print(\"Forth test passed\")\n",
        "\n",
        "    # Fifth test\n",
        "    sample = dataset[80000]\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "    answer = sample['answer']\n",
        "    start_pos = sample['start_pos']\n",
        "\n",
        "    input_ids, token_type_ids, start_pos, end_pos = squad_features(context, question, answer, start_pos, tokenizer)\n",
        "    \n",
        "    assert len(input_ids) == 165, \\\n",
        "        \"Tokenized result does not match the expected result.\"\n",
        "\n",
        "    assert tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1]) == tokenizer.tokenize(answer), \\\n",
        "        \"The start and end tokens do not point to the answer position.\"\n",
        "\n",
        "    print(\"Fifth test passed\")\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "def test_squadFeatureDataset(dataset):\n",
        "    print(\"Testing\")\n",
        "    dataset = SquadFeatureDataset(dataset, bert_type='bert-base-uncased', lazy=True)\n",
        "\n",
        "    input_ids, token_type_ids, start_pos, end_pos = dataset[0]\n",
        "    print(\"input_ids:\", input_ids)\n",
        "    print(\"token_type_ids:\", token_type_ids)\n",
        "    print(\"start_pos:\", start_pos)\n",
        "    print(\"end_pos:\", end_pos)\n",
        "\n",
        "    print(\"The test passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = SquadDataset()\n",
        "\n",
        "    test_squadDataSet(dataset)\n",
        "    test_squadFeatureExtractor(dataset)\n",
        "    test_squadFeatureDataset(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data samples from raw SQuAD json file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442/442 [00:00<00:00, 576.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset length:  87474\n",
            "Dataset Test Cases\n",
            "First test passed\n",
            "Second test passed\n",
            "Third test passed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Forth test passed\n",
            "Fifth test passed\n",
            "All tests passed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3be127d01c084978b64e1d2e2bf0ccb0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "473e84af145c45dab5132a9dfe992045",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "First test passed!\n",
            "Second test passed!\n",
            "Third test passed\n",
            "Forth test passed\n",
            "Fifth test passed\n",
            "All tests passed!\n",
            "Testing\n",
            "input_ids: [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102]\n",
            "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "start_pos: 130\n",
            "end_pos: 137\n",
            "The test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8OJDFG7r75Y"
      },
      "source": [
        "# Training Model\n",
        "\n",
        "Below cell consists of training function for the Squad Question Answering BERT model. For our model, we have used \n",
        "<br>**5 epochs** <br>\n",
        "**batch size of 10**<br>\n",
        "**learning rate of 5e-5** <br> **bert type: 'bert-base-uncased'** . <br>\n",
        "\n",
        "Initially, we have trained the model of 3 epochs with a 16GB RAM, it took close to 40 hours to complete and save the checkpoint. So in order to test for 5 epochs, it would take us days to get it trained. Hence we have bought Colab pro which offers us 26 GB RAM and T4 GPU. \n",
        "<br> \n",
        "Now with Colab Pro,we have tried for 5 epochs and it took close to 2-3 hours for training it. Hence, it is recommended to run the below cell on a colab pro account or a faster GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "408c2db619674c14a6c1a0e4382b3d04",
            "39ffeb21145142b09f6805f210623126",
            "0aa750824516474590a96906408d85d6",
            "56f3ba4d99f849929621a3eaacf709f7",
            "b51506652dae494ab794a863912cf417",
            "641a0897725c47bea5577844f70234c5",
            "e807f15c908b4c0cb0023a82732cd6a9",
            "bc0a5de7a13344c3b241072e578f2fd0",
            "f9bd3147346d46f880a2487265b125c2",
            "c28a9bd3b8c34fefbee8d23279a7562c",
            "d8e405936fa54fba9e8b6484850d2b7b",
            "7b581d1e8cdf4a5fb44da746f6d52ac3",
            "519cab4e2e4b4a78a924049d8639cb0c",
            "970cf262def34ae98b7c1e99cc18c739",
            "8910da74e0df43a38b7f7e1f3c8213ff",
            "02e56b0a3e9e494eb4365bf695dfdb1b"
          ]
        },
        "id": "BwCvsdXJ8KFt",
        "outputId": "747d9160-d2f6-43b2-afb8-6dd5c21de8ca"
      },
      "source": [
        "#Training the BERT model\n",
        "from typing import List, Tuple\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Sampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "def train():\n",
        "    \"\"\" Training function for Squad QA BERT model\n",
        "    Explanation: https://medium.com/@davidlmorton/increasing-mini-batch-size-without-increasing-memory-6794e10db672\n",
        "    Useful readings: https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/ \n",
        "    \"\"\" \n",
        "    epochs = 5\n",
        "    learning_rate = 5e-5\n",
        "    batch_size = 10\n",
        "    bert_type = 'bert-base-uncased' \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Change the lazy option if you want fast debugging.\n",
        "    dataset = SquadFeatureDataset(SquadDataset(), bert_type=bert_type, lazy=False) \n",
        "\n",
        "    model = BertForSquad.from_pretrained(bert_type)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "    batch_sampler = SquadBucketSampler(dataset, batch_size, shuffle=True)\n",
        "    data_loader = DataLoader(dataset, batch_sampler=batch_sampler, collate_fn=squad_feature_assemble)\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        batch_loss = 0.0\n",
        "        for input_ids, attention_mask, token_type_ids, start_pos, end_pos in tqdm(data_loader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            start_pos = start_pos.to(device)\n",
        "            end_pos = end_pos.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            start_logits, end_logits = model(input_ids, attention_mask, token_type_ids)\n",
        "            start_loss = loss_fct(start_logits, start_pos)\n",
        "            end_loss = loss_fct(end_logits, end_pos)\n",
        "            loss = start_loss + end_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            batch_loss += loss.item()\n",
        "            del start_logits, end_logits, loss # MemoryError\n",
        "        losses.append(batch_loss)\n",
        "\n",
        "    # Saving the model in the checkpoint folder.\n",
        "    # After the training cell is executed, you would be able to see a folder which consists of pytorch model bin file along with config.json \n",
        "    model.save_pretrained('./checkpoint')\n",
        "\n",
        "def squad_feature_assemble(\n",
        "    samples: List[Tuple[List[int], List[int], int, int]]):\n",
        "    input_ids, token_type_ids, start_pos, end_pos = zip(*samples)\n",
        "    attention_mask = [[1] * len(input_id) for input_id in input_ids]\n",
        "\n",
        "    input_ids = pad_sequence([torch.Tensor(input_id).to(torch.long) for input_id in input_ids], \\\n",
        "                             padding_value=0, batch_first=True)\n",
        "    token_type_ids = pad_sequence([torch.Tensor(token_type_id).to(torch.long) for token_type_id in token_type_ids], \\\n",
        "                                  padding_value=1, batch_first=True)\n",
        "    attention_mask = pad_sequence([torch.Tensor(mask).to(torch.long) for mask in attention_mask], \\\n",
        "                                  padding_value=0, batch_first=True)\n",
        "\n",
        "    start_pos = torch.Tensor(start_pos).to(torch.long)\n",
        "    end_pos = torch.Tensor(end_pos).to(torch.long)\n",
        "    \n",
        "    return input_ids, attention_mask, token_type_ids, start_pos, end_pos\n",
        "\n",
        "class SquadBucketSampler(Sampler):\n",
        "    \"\"\" Squad dataset bucketed batch sampler\n",
        "    squad_feature_dataset(squad_dataset, lazy=False)\n",
        "    batch_sampler = SquadBucketSampler(squad_feature_dataset, batch_size, shuffle=True)\n",
        "    data_loader = DataLoader(squad_feature_dataset, ..., batch_size=1, batch_sampler=batch_sampler, ...)\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset: SquadFeatureDataset, batch_size, shuffle=False):\n",
        "        super().__init__(dataset)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        _, indices = zip(*sorted((len(input_ids), index) for index, (input_ids, _, _, _) in enumerate(tqdm(dataset, desc=\"Bucketing\"))))\n",
        "        self.batched_indices = [indices[index: index+batch_size] for index in range(0, len(indices), batch_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batched_indices)\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.batched_indices)\n",
        "\n",
        "        for batch in self.batched_indices:\n",
        "            yield batch\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data sample from raw SQuAD json file... >>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442/442 [00:00<00:00, 821.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset length:  87474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87474/87474 [01:21<00:00, 1070.20it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "408c2db619674c14a6c1a0e4382b3d04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9bd3147346d46f880a2487265b125c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSquad: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSquad from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSquad from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSquad were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Bucketing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87474/87474 [00:00<00:00, 753908.42it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8748/8748 [28:39<00:00,  5.09it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8748/8748 [28:36<00:00,  5.10it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8748/8748 [28:34<00:00,  5.10it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8748/8748 [28:34<00:00,  5.10it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8748/8748 [28:33<00:00,  5.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrqzKpI3xUZ-"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Determining the exact match score along with the f1 score. **F1 Score** is used to measure the search, document classification performance. <br>\n",
        "\n",
        "**Exact matching score**  compares the ground truth answer to prediction and gives us an estimated score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HoWoJl8K1Bu"
      },
      "source": [
        "\"\"\" Official evaluation script for v1.1 of the SQuAD dataset. \"\"\"\n",
        "from __future__ import print_function\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "\n",
        "def normalize_answer(s):\n",
        "    # Lower text and remove punctuation, articles and extra whitespace\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def F1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "def evaluate(dataset, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    message = 'Unanswered question ' + qa['id'] + \\\n",
        "                              ' will receive score 0.'\n",
        "                    print(message, file=sys.stderr)\n",
        "                    continue\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                prediction = predictions[qa['id']]\n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    F1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n2bI88V-ErM"
      },
      "source": [
        "# Inference\n",
        "\n",
        "The inference function probability tensor for the start , end position and start,end indices of the context. It finds the start and end positions of the answer which maximizes\n",
        " P(start, end | context_start_pos <= start <= end <= context_end_pos) . <br>\n",
        "\n",
        "\n",
        "\n",
        " In the inference answer function, we find the original words in the context by the start and end token positions of the answer and implement the inference function from the context and answer token position. <br>\n",
        "\n",
        "\n",
        "Reference link for transformers : https://huggingface.co/transformers/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpT3Kyt0CqUB",
        "outputId": "030eb483-7e67-4b4f-fa82-4ecc3ca3a620"
      },
      "source": [
        "# inference.py\n",
        "\n",
        "from typing import List\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from transformers import BertTokenizerFast\n",
        "from tqdm import trange\n",
        "\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "bert_type = 'bert-base-uncased' \n",
        "\n",
        "def inference_start_end(\n",
        "    start_probs: torch.Tensor,\n",
        "    end_probs: torch.Tensor,\n",
        "    context_start_pos: int,\n",
        "    context_end_pos: int):\n",
        "    assert start_probs.sum().allclose(torch.scalar_tensor(1.))\n",
        "    assert end_probs.sum().allclose(torch.scalar_tensor(1.))  \n",
        "    start_pos, end_pos = 0, 0\n",
        "    start_end_probs = torch.stack((start_probs, end_probs))\n",
        "\n",
        "    _, start_pos = start_probs.max(-1)\n",
        "    start_pos = start_pos.item()\n",
        "    while True:\n",
        "        if start_pos < context_start_pos or start_pos == context_end_pos:\n",
        "            start_probs[start_pos] = 0.0\n",
        "            _, start_pos = start_probs.max(-1)\n",
        "        else:\n",
        "            start_end_probs = torch.triu(start_end_probs, diagonal=start_pos)\n",
        "            _, end_pos = start_end_probs[1].max(-1)\n",
        "            break\n",
        "    if end_pos > context_end_pos:\n",
        "        start_end_probs[1][end_pos] = 0.0\n",
        "        _, end_pos = start_end_probs[1].max(-1)\n",
        "    \n",
        "    return start_pos, end_pos\n",
        "\n",
        "def inference_answer(\n",
        "    question: str,\n",
        "    context: str,\n",
        "    input_ids: List[int],\n",
        "    token_type_ids: List[int],\n",
        "    start_pos: int,\n",
        "    end_pos: int,\n",
        "    tokenizer: BertTokenizerFast) -> str:\n",
        "\n",
        "    answer = input_ids[start_pos: end_pos+1]\n",
        "    answer: str = tokenizer.decode(answer)\n",
        "    encoded_context = tokenizer.encode_plus(question,context, return_offsets_mapping=True)\n",
        "    answer_char_pos = encoded_context['offset_mapping'][start_pos:end_pos+1]\n",
        "    answer = context[answer_char_pos[0][0] : answer_char_pos[-1][1]]\n",
        "\n",
        "    return answer\n",
        "\n",
        "def inference_model(\n",
        "    model: BertForSquad,\n",
        "    tokenizer: BertTokenizerFast,\n",
        "    context: str,\n",
        "    question: str,\n",
        "    input_ids: List[int],\n",
        "    token_type_ids: List[int]\n",
        ") -> str:\n",
        "    \n",
        "    anwser: str = None\n",
        "    model.eval()\n",
        "\n",
        "    encoded_input = tokenizer.encode_plus(question, context)\n",
        "    input_ids = torch.tensor([encoded_input['input_ids']], device=device)\n",
        "    attention_mask = torch.tensor([encoded_input['attention_mask']], device=device)\n",
        "    token_type_ids = torch.tensor([encoded_input['token_type_ids']], device=device)\n",
        "    start_logits, end_logits = model(input_ids, attention_mask, token_type_ids)\n",
        "    start_probs = F.softmax(start_logits, dim=-1)\n",
        "    end_probs = F.softmax(end_logits, dim=-1)\n",
        "    context_start_pos = encoded_input['input_ids'].index(tokenizer.sep_token_id) + 1\n",
        "    context_end_pos = len(encoded_input['input_ids']) - 2\n",
        "    start_pos, end_pos = inference_start_end(start_probs, end_probs, context_start_pos, context_end_pos)\n",
        "    answer = inference_answer(question, context, encoded_input['input_ids'], encoded_input['token_type_ids'], start_pos, end_pos, tokenizer)\n",
        "    return answer \n",
        "\n",
        "# Testing\n",
        "\n",
        "def test_inference_start_end_pos(tokenizer):    \n",
        "    # First test\n",
        "    input_tokens = ['[CLS]', 'this', 'is', 'a', 'question', '.', '[SEP]', 'this', 'is', 'an', 'answer', '.', '[SEP]']\n",
        "    start_probs  = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.0,     0.0,    0.0,  0.1,  0.8,      0.1, 0.0,     0.0]\n",
        "    end_probs    = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.0,     0.0,    0.0,  0.0,  0.1,      0.8, 0.1,     0.0]\n",
        "    context_start_pos = input_tokens.index('[SEP]') + 1\n",
        "    context_end_pos = len(input_tokens) - 2\n",
        "\n",
        "    start_probs = torch.Tensor(start_probs)\n",
        "    end_probs = torch.Tensor(end_probs)\n",
        "    start_pos, end_pos = inference_start_end(start_probs, end_probs, context_start_pos, context_end_pos)\n",
        "    answer = input_tokens[start_pos: end_pos+1] \n",
        "\n",
        "    assert answer == ['an', 'answer'], \\\n",
        "        \"Your infered position is different from the expected position.\"\n",
        "    \n",
        "    print(\"First test passed\")\n",
        "\n",
        "    # Second test\n",
        "    input_tokens = ['[CLS]', 'this', 'is', 'a', 'question', '.', '[SEP]', 'this', 'is', 'an', 'answer', '.', '[SEP]']\n",
        "    start_probs  = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.0,     0.0,    0.0,  0.1,  0.8,      0.1, 0.0,     0.0]\n",
        "    end_probs    = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.0,     0.0,    0.0,  0.6,  0.1,      0.3, 0.0,     0.0]\n",
        "    context_start_pos = input_tokens.index('[SEP]') + 1\n",
        "    context_end_pos = len(input_tokens) - 2\n",
        "\n",
        "    start_probs = torch.Tensor(start_probs)\n",
        "    end_probs = torch.Tensor(end_probs)\n",
        "    start_pos, end_pos = inference_start_end(start_probs, end_probs, context_start_pos, context_end_pos)\n",
        "    answer = input_tokens[start_pos: end_pos+1] \n",
        "\n",
        "    assert answer == ['an', 'answer'], \\\n",
        "        \"Your infered position is different from the expected position.\"\n",
        "    print(\"Second test passed\")\n",
        "\n",
        "    # third test\n",
        "    input_tokens = ['[CLS]', 'this', 'is', 'a', 'question', '.', '[SEP]', 'this', 'is', 'an', 'answer', '.', '[SEP]']\n",
        "    start_probs  = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.0,     0.0,    0.1,  0.2,  0.3,      0.1, 0.3,     0.0]\n",
        "    end_probs    = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.0,     0.0,    0.4,  0.2,  0.1,      0.2, 0.1,     0.0]\n",
        "    context_start_pos = input_tokens.index('[SEP]') + 1\n",
        "    context_end_pos = len(input_tokens) - 2\n",
        "\n",
        "    start_probs = torch.Tensor(start_probs)\n",
        "    end_probs = torch.Tensor(end_probs)\n",
        "    start_pos, end_pos = inference_start_end(start_probs, end_probs, context_start_pos, context_end_pos)\n",
        "    answer = input_tokens[start_pos: end_pos+1] \n",
        "\n",
        "    assert answer == ['an', 'answer'], \\\n",
        "        \"Your infered position is different from the expected position.\"\n",
        "    print(\"Third test passed\")\n",
        "\n",
        "    # forth test\n",
        "    input_tokens = ['[CLS]', 'this', 'is', 'a', 'question', '.', '[SEP]', 'this', 'is', 'an', 'answer', '.', '[SEP]']\n",
        "    start_probs  = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.3,     0.3,    0.0,  0.1,  0.2,      0.1, 0.0,     0.0]\n",
        "    end_probs    = [    0.0,    0.0,  0.0, 0.0,        0.0, 0.0,     0.0,    0.0,  0.2,  0.0,      0.2, 0.0,     0.6]\n",
        "    context_start_pos = input_tokens.index('[SEP]') + 1\n",
        "    context_end_pos = len(input_tokens) - 2\n",
        "\n",
        "    start_probs = torch.Tensor(start_probs)\n",
        "    end_probs = torch.Tensor(end_probs)\n",
        "    start_pos, end_pos = inference_start_end(start_probs, end_probs, context_start_pos, context_end_pos)\n",
        "    answer = input_tokens[start_pos: end_pos+1] \n",
        "\n",
        "    assert answer == ['an', 'answer'], \\\n",
        "        \"Your infered position is different from the expected position.\"\n",
        "    print(\"Forth test passed\")\n",
        "\n",
        "def test_inference_answer(tokenizer):\n",
        "    print(\"======Answer Inference Test Case======\")\n",
        "\n",
        "    # First test\n",
        "    context = \"The example answer was $5.00 USD.\"\n",
        "    question = \"What was the answer?\"\n",
        "    answer = \"$5.00 USD\"\n",
        "    start_pos = context.find(answer)\n",
        "\n",
        "    input_ids, token_type_ids, start_pos, end_pos = squad_features(context, question, answer, start_pos, tokenizer)\n",
        "    prediction = inference_answer(question, context, input_ids, token_type_ids, start_pos, end_pos, tokenizer)\n",
        "    \n",
        "    if prediction == \"$ 5. 00 usd\":\n",
        "        print(\"Skip the test. You get no score.\")\n",
        "        return\n",
        "\n",
        "    assert prediction == answer, \\\n",
        "        \"Your answer is different from the expected answer.\"\n",
        "\n",
        "    print(\"First test passed\")\n",
        "\n",
        "    # Second test\n",
        "    context = \"The speed of the light is 299,794,458 m/s.\"\n",
        "    question = \"What is the speed of the light?\"\n",
        "    answer = \"299,794,458 m/s\"\n",
        "    start_pos = context.find(answer)\n",
        "\n",
        "    input_ids, token_type_ids, start_pos, end_pos = squad_features(context, question, answer, start_pos, tokenizer)\n",
        "    prediction = inference_answer(question, context, input_ids, token_type_ids, start_pos, end_pos, tokenizer)\n",
        "\n",
        "    assert prediction == answer, \\\n",
        "        \"Your answer is different from the expected answer.\"\n",
        "\n",
        "    print(\"Second test passed\")\n",
        "\n",
        "# Analysis\n",
        "\n",
        "def qualitative_analysis(tokenizer, model):    \n",
        "    \n",
        "\n",
        "    print(\"===================================================================\")\n",
        "\n",
        "    # Testing for 3 contexts with questions, it's answers and prediction\n",
        "    context = \"The first buildings of the University of Chicago campus, which make up what is now known as the Main Quadrangles, were part of a \\\n",
        "            master plan conceived by two University of Chicago trustees and plotted by Chicago architect Henry Ives Cobb. The Main Quadrangles\\\n",
        "            consist of six quadrangles, each surrounded by buildings, bordering one larger quadrangle. \\\n",
        "            The buildings of the Main Quadrangles were designed by Cobb, Shepley, Rutan and Coolidge, Holabird & Roche, and \\\n",
        "            other architectural firms in a mixture of the Victorian Gothic and Collegiate Gothic styles, patterned on the colleges \\\n",
        "            of the University of Oxford. (Mitchell Tower, for example, is modeled after Oxford's Magdalen Tower, \\\n",
        "            and the university Commons, Hutchinson Hall, replicates Christ Church Hall.)\"\n",
        "\n",
        "    question = \"How many quadrangles does the Main Quadrangles have?\"\n",
        "\n",
        "    plausible_answers = [\"six\", \"six quadrangles\"]\n",
        "    start_pos = context.find(plausible_answers[0])\n",
        "    input_ids, token_type_ids, _, _ = squad_features(context, question, plausible_answers[0], start_pos, tokenizer)\n",
        "\n",
        "    prediction = inference_model(model, tokenizer, context, question, input_ids, token_type_ids)\n",
        "\n",
        "    print(\"------------Sample 1------------\")\n",
        "    print(f\"Context: {context}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Plausible Answers: {plausible_answers}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(\"===================================================================\")\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    context= '''\n",
        "    Southern California, often abbreviated SoCal, is a geographic and cultural region that generally comprises California's southernmost 10 \n",
        "    counties. The region is traditionally described as \"eight counties\", based on demographics and economic ties: Imperial, \n",
        "    Los Angeles, Orange, Riverside, San Bernardino, San Diego, Santa Barbara, and Ventura. \n",
        "    The more extensive 10-county definition, including Kern and San Luis Obispo counties, is also used based on historical \n",
        "    political divisions. Southern California is a major economic center for the state of California and the United States.\n",
        "    '''\n",
        "    question = \"What is a major importance of Southern California in relation to California and the United States?\"\n",
        "    \n",
        "    plausible_answers = [\"economic center\", \"major \"]\n",
        "    plausible_answers = [\"economic center\", \"major economic center\"]\n",
        "    start_pos = context.find(plausible_answers[0])\n",
        "    input_ids, token_type_ids, _, _ = squad_features(context, question, plausible_answers[0], start_pos, tokenizer)\n",
        "\n",
        "    prediction = inference_model(model, tokenizer, context, question, input_ids, token_type_ids)\n",
        "\n",
        "    print(\"------------Sample 2------------\")\n",
        "    print(f\"Context: {context}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Plausible Answers: {plausible_answers}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(\"===================================================================\")\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    context='''\n",
        "    The immune system is a system of many biological structures and processes within an organism that protects against disease. \n",
        "    To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, \n",
        "    and distinguish them from the organism's own healthy tissue. In many species, the immune system can be classified into subsystems,\n",
        "    such as the innate immune system versus the adaptive immune system, or humoral immunity versus cell-mediated immunity. \n",
        "    In humans, the bloodâ€“brain barrier, bloodâ€“cerebrospinal fluid barrier, and similar fluidâ€“brain barriers separate the \n",
        "    peripheral immune system from the neuroimmune system which protects the brain.\n",
        "    '''\n",
        "    question = \"What are the agents the immune system detects known as?\"\n",
        "    plausible_answers = [\"pathogens\", \"pathogens\"]\n",
        "    start_pos = context.find(plausible_answers[0])\n",
        "    input_ids, token_type_ids, _, _ = squad_features(context, question, plausible_answers[0], start_pos, tokenizer)\n",
        "\n",
        "    prediction = inference_model(model, tokenizer, context, question, input_ids, token_type_ids)\n",
        "\n",
        "    print(\"------------Sample 3------------\")\n",
        "    print(f\"Context: {context}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Plausible Answers: {plausible_answers}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(\"===================================================================\")\n",
        "    \n",
        "\n",
        "def quantative_analysis(tokenizer, model):\n",
        "    dataset = SquadDataset('data/dev-v1.1-TA.json')\n",
        "    dataset = SquadFeatureDataset(dataset, bert_type=bert_type, lazy=True, return_sample=True, eval=True)\n",
        "\n",
        "    answers = dict()\n",
        "\n",
        "    for index in trange(len(dataset), desc=\"Answering\"):\n",
        "        (input_ids, token_type_ids, _, _), sample = dataset[index]\n",
        "        answers[sample['id']] = \\\n",
        "                inference_model(model, tokenizer, sample['context'], sample['question'], input_ids, token_type_ids)\n",
        "\n",
        "    with open('dev-v1.1-TA-answers.json', mode='w') as f:\n",
        "        json.dump(answers, f)\n",
        "    \n",
        "    with open('data/dev-v1.1-TA.json', mode='r') as f:\n",
        "        dataset = json.load(f)['data']\n",
        "\n",
        "    results = evaluate(dataset, answers)\n",
        "    print(f\"Exact Match: {results['exact_match']}. This should be higher than 60.0.\")\n",
        "    print(f\"F1 score: {results['f1']}. This should be higher than 70.0.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(bert_type)\n",
        "\n",
        "    test_inference_start_end_pos(tokenizer)\n",
        "    test_inference_answer(tokenizer)\n",
        "\n",
        "    model = BertForSquad.from_pretrained('/content/drive/MyDrive/checkpoint')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    qualitative_analysis(tokenizer, model)\n",
        "    quantative_analysis(tokenizer, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First test passed\n",
            "Second test passed\n",
            "Third test passed\n",
            "Forth test passed\n",
            "======Answer Inference Test Case======\n",
            "First test passed\n",
            "Second test passed\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 1994.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "===================================================================\n",
            "------------Sample 1------------\n",
            "Context: The first buildings of the University of Chicago campus, which make up what is now known as the Main Quadrangles, were part of a             master plan conceived by two University of Chicago trustees and plotted by Chicago architect Henry Ives Cobb. The Main Quadrangles            consist of six quadrangles, each surrounded by buildings, bordering one larger quadrangle.             The buildings of the Main Quadrangles were designed by Cobb, Shepley, Rutan and Coolidge, Holabird & Roche, and             other architectural firms in a mixture of the Victorian Gothic and Collegiate Gothic styles, patterned on the colleges             of the University of Oxford. (Mitchell Tower, for example, is modeled after Oxford's Magdalen Tower,             and the university Commons, Hutchinson Hall, replicates Christ Church Hall.)\n",
            "Question: How many quadrangles does the Main Quadrangles have?\n",
            "Plausible Answers: ['six', 'six quadrangles']\n",
            "Prediction: six\n",
            "===================================================================\n",
            "------------Sample 2------------\n",
            "Context: \n",
            "    Southern California, often abbreviated SoCal, is a geographic and cultural region that generally comprises California's southernmost 10 \n",
            "    counties. The region is traditionally described as \"eight counties\", based on demographics and economic ties: Imperial, \n",
            "    Los Angeles, Orange, Riverside, San Bernardino, San Diego, Santa Barbara, and Ventura. \n",
            "    The more extensive 10-county definition, including Kern and San Luis Obispo counties, is also used based on historical \n",
            "    political divisions. Southern California is a major economic center for the state of California and the United States.\n",
            "    \n",
            "Question: What is a major importance of Southern California in relation to California and the United States?\n",
            "Plausible Answers: ['economic center', 'major economic center']\n",
            "Prediction: major economic center\n",
            "===================================================================\n",
            "------------Sample 3------------\n",
            "Context: \n",
            "    The immune system is a system of many biological structures and processes within an organism that protects against disease. \n",
            "    To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, \n",
            "    and distinguish them from the organism's own healthy tissue. In many species, the immune system can be classified into subsystems,\n",
            "    such as the innate immune system versus the adaptive immune system, or humoral immunity versus cell-mediated immunity. \n",
            "    In humans, the bloodâ€“brain barrier, bloodâ€“cerebrospinal fluid barrier, and similar fluidâ€“brain barriers separate the \n",
            "    peripheral immune system from the neuroimmune system which protects the brain.\n",
            "    \n",
            "Question: What are the agents the immune system detects known as?\n",
            "Plausible Answers: ['pathogens', 'pathogens']\n",
            "Prediction: pathogens\n",
            "===================================================================\n",
            "Extracting data samples from raw SQuAD json file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset length:  10512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Answering: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10512/10512 [04:17<00:00, 40.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exact Match: 75.31392694063926. This should be higher than 60.0.\n",
            "F1 score: 84.34708241748314. This should be higher than 70.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMstEw0tVMVk",
        "outputId": "42affef4-8e5e-4e97-cbda-942857064c95"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    expected_version = '1.1'\n",
        "    with open('data/dev-v1.1-TA.json') as dataset_file:\n",
        "        dataset_json = json.load(dataset_file)\n",
        "        if (dataset_json['version'] != expected_version):\n",
        "            print('Evaluation expects v-' + expected_version +\n",
        "                  ', but got dataset with v-' + dataset_json['version'],\n",
        "                  file=sys.stderr)\n",
        "        dataset = dataset_json['data']\n",
        "    with open('dev-v1.1-TA-answers.json') as prediction_file:\n",
        "        predictions = json.load(prediction_file)\n",
        "    print(json.dumps(evaluate(dataset, predictions)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"exact_match\": 75.31392694063926, \"f1\": 84.34708241748314}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ifMV4ktEdRs",
        "outputId": "a0997cbb-83f2-4095-d6b1-e49526ac7882"
      },
      "source": [
        "context='''\n",
        "One of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on \n",
        "mechanics, Philo of Byzantium. In his work Pneumatica, Philo observed that inverting a vessel over a burning candle and surrounding \n",
        "the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel \n",
        "were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later \n",
        "Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration.\n",
        "'''\n",
        "question = \"What inventor built on to the findings of Philo of Byzantium?\"\n",
        "\n",
        "plausible_answers = [\"Leonardo da Vinci\", \"Leonardo da Vinci\"]\n",
        "start_pos = context.find(plausible_answers[0])\n",
        "input_ids, token_type_ids, _, _ = squad_features(context, question, plausible_answers[0], start_pos, tokenizer)\n",
        "\n",
        "prediction = inference_model(model, tokenizer, context, question, input_ids, token_type_ids)\n",
        "\n",
        "# print(\"Sample Test\")\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Plausible Answers: {plausible_answers}\")\n",
        "print(f\"Prediction: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context: \n",
            "One of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on \n",
            "mechanics, Philo of Byzantium. In his work Pneumatica, Philo observed that inverting a vessel over a burning candle and surrounding \n",
            "the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel \n",
            "were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later \n",
            "Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration.\n",
            "\n",
            "Question: What inventor built on to the findings of Philo of Byzantium?\n",
            "Plausible Answers: ['Leonardo da Vinci', 'Leonardo da Vinci']\n",
            "Prediction: Leonardo da Vinci\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYs5NxlaBptg"
      },
      "source": [
        "# Experiment and Insights\n",
        "\n",
        "We experimented with learning rate and found 5e-5 as the optimal value. This is because though the Exact Match and F1 scores were high at lower learning rates, training took very long and after 5e-5 the scores starting decreasing due to the fact that model misses the optimum at high learning rates. \n",
        "\n",
        "We plotted the graph using values in a list obtained after training models separately as training all the 6 models in an iteration would crash the colab notebook. Also, the values are provided below for reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jghoU6v_BCzl"
      },
      "source": [
        "# le:1e-5 = {\"exact_match\": 78.6910197869102, \"f1\": 86.77091531034209}\n",
        "# le:2e-5 = {\"exact_match\": 78.17732115677322, \"f1\": 86.5129027278223}\n",
        "# le:3e-5 = {\"exact_match\": 78.02511415525115, \"f1\": 86.34739133228362}\n",
        "# le:4e-5 = {\"exact_match\": 74.57191780821918, \"f1\": 83.85115054558722}\n",
        "# le:5e-5 = {\"exact_match\": 75.31392694063926, \"f1\": 84.34708241748314}\n",
        "# le:6e-5 = {\"exact_match\": 71.9558599695586, \"f1\": 81.61010039043859}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKDf7vC6BC2e"
      },
      "source": [
        "lr = [1e-5, 2e-5, 3e-5, 4e-5, 5e-5, 6e-5]\n",
        "em = [78.69, 78.18, 78.03, 74.57, 75.31, 71.96]\n",
        "f1 = [86.77, 86.51, 86.35, 83.85, 84.34, 81.61]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "H4kWl5R-BFxI",
        "outputId": "529ac428-9310-4985-b9e0-18c75efdc43e"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Learning Rate vs Exact Match and F1 Score\")\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.plot(lr, em, label = 'Exact Match')\n",
        "plt.plot(lr, f1, label = 'F1 Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1NQWhmqFkGP",
        "outputId": "fddda3c9-7053-4177-aa8d-d7825bc932dc"
      },
      "source": [
        "!pip install flask-ngrok \n",
        "# >> dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJRle3k411h5"
      },
      "source": [
        "# User Interface\n",
        "\n",
        "A simple GUI has been developed, where a user can enter any context and question and a answer will be generated. To run the below cell, flask-ngrok needs to be imported. <br>\n",
        "\n",
        "Whenever the below cell is run, a different link is generated. The context and the question entered in the html are sent via POST method and it is then passed as an argument in the inference model to predict the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8L98iAwGwBM",
        "outputId": "121c3a30-bc2f-4342-d5f9-669bd41f5568"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template\n",
        "from flask import request\n",
        "import flask    \n",
        "app = Flask(__name__, template_folder='drive/My Drive/templates')\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "\n",
        "    return render_template('index.html')  \n",
        "@app.route('/', methods=['POST'])\n",
        "def my_form_post():\n",
        "    context = request.form['context']\n",
        "    question=request.form['question']\n",
        "    # plausible_answers = [\"Computational complexity theory\", \"Computational complexity theory\"]\n",
        "    # input_ids, token_type_ids, _, _ = squad_features(context, question, plausible_answers[0], start_pos, tokenizer)\n",
        "    prediction = inference_model(model, tokenizer, context, question, input_ids, token_type_ids)\n",
        "    \n",
        "    return render_template('index.html',question=question, answer=prediction)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://4ccdb869ea23.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [08/Dec/2020 02:26:14] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [08/Dec/2020 02:26:15] \"\u001b[33mGET /static/logo.png HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [08/Dec/2020 02:26:43] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lro92gQGGp69"
      },
      "source": [
        "# from google.colab.output import eval_js\n",
        "# print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}